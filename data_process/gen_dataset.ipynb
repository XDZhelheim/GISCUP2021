{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/cseadmin/undergrad1/dz/GISCUP2021/data_process'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import os, random, csv, datetime, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dir=\"../data/train/\"\n",
    "test_dir=\"../data/test/\"\n",
    "\n",
    "link_dir=\"../data/\"\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feats(task, head, path, nl, link_attr, link_time_attr, driver_attr):\n",
    "########################\n",
    "###head features\n",
    "########################\n",
    "    head['speed'] = head['distance']/head['simple_eta']\n",
    "    head['speed_avg'] = head['slice_id'].map(head.groupby('slice_id')['speed'].mean())\n",
    "    head['speed_std'] = head['slice_id'].map(head.groupby('slice_id')['speed'].std())\n",
    "    head['speed_min'] = head['slice_id'].map(head.groupby('slice_id')['speed'].min())\n",
    "    head['speed_max'] = head['slice_id'].map(head.groupby('slice_id')['speed'].max())\n",
    "    head['volume'] = head['slice_id'].map(head.groupby('slice_id').size())\n",
    "    head['old_driver'] = head['driver_id'].map(head.groupby('driver_id').size())\n",
    "\n",
    "    head['driver_avg'] = head['driver_id'].map(driver_attr['avg'])\n",
    "    head['driver_std'] = head['driver_id'].map(driver_attr['std'])\n",
    "    head['driver_min'] = head['driver_id'].map(driver_attr['min'])\n",
    "    head['driver_max'] = head['driver_id'].map(driver_attr['max'])\n",
    "    head['driver_cnt'] = head['driver_id'].map(driver_attr['count'])\n",
    "    \n",
    "    head.set_index('order_id', inplace=True)\n",
    "    \n",
    "    path['link_eta'] = path['link_time']*path['link_ratio']\n",
    "    path['cum_time'] = (path.groupby('order_id')['link_eta'].cumsum()/300).astype('int')\n",
    "    link = path[path['cross_flag']==0]\n",
    "    cross = path[path['cross_flag']==1]\n",
    "########################\n",
    "###link features\n",
    "########################\n",
    "    link = link[link['link_time']>0]\n",
    "    link['link_id'] = link['link_id'].astype('int')\n",
    "    link['link_type'] = link['link_id'].map(link_attr['link_label']).fillna(0).astype('int')\n",
    "    link['next_link'] = link['link_id'].map(nl).fillna(0).astype('int')\n",
    "    link['slice_id'] = link['order_id'].map(head['slice_id'])\n",
    "\n",
    "    link[\"link_time_std\"] = link[\"link_id\"].map(link_time_attr[\"std\"])\n",
    "    link[\"link_time_avg\"] = link[\"link_id\"].map(link_time_attr[\"avg\"])\n",
    "    link[\"link_time_min\"] = link[\"link_id\"].map(link_time_attr[\"min\"])\n",
    "    link[\"link_time_max\"] = link[\"link_id\"].map(link_time_attr[\"max\"])\n",
    "    link[\"link_time_count\"] = link[\"link_id\"].map(link_time_attr[\"count\"])\n",
    "    \n",
    "    gl = link.groupby('order_id')\n",
    "    head['link_no'] = head.index.map(gl.size())\n",
    "    head['link_time_sum'] = head.index.map(gl['link_eta'].sum())\n",
    "    head['link_time_avg'] = head.index.map(gl['link_time'].mean())\n",
    "    head['link_time_std'] = head.index.map(gl['link_time'].std())\n",
    "    head['link_time_max'] = head.index.map(gl['link_time'].max())\n",
    "    head['link_time_min'] = head.index.map(gl['link_time'].min())\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "    head['link_time_std_sum'] = head.index.map(gl['link_time_std'].sum())\n",
    "    head['link_time_std_avg'] = head.index.map(gl['link_time_std'].mean())\n",
    "    head['link_time_std_std'] = head.index.map(gl['link_time_std'].std())\n",
    "    head['link_time_std_min'] = head.index.map(gl['link_time_std'].min())\n",
    "    head['link_time_std_max'] = head.index.map(gl['link_time_std'].max())\n",
    "\n",
    "    head['link_time_avg_sum'] = head.index.map(gl['link_time_avg'].sum())\n",
    "    head['link_time_avg_avg'] = head.index.map(gl['link_time_avg'].mean())\n",
    "    head['link_time_avg_std'] = head.index.map(gl['link_time_avg'].std())\n",
    "    head['link_time_avg_min'] = head.index.map(gl['link_time_avg'].min())\n",
    "    head['link_time_avg_max'] = head.index.map(gl['link_time_avg'].max())\n",
    "\n",
    "    head['link_time_max_sum'] = head.index.map(gl['link_time_max'].sum())\n",
    "    head['link_time_max_avg'] = head.index.map(gl['link_time_max'].mean())\n",
    "    head['link_time_max_std'] = head.index.map(gl['link_time_max'].std())\n",
    "    head['link_time_max_min'] = head.index.map(gl['link_time_max'].min())\n",
    "    head['link_time_max_max'] = head.index.map(gl['link_time_max'].max())\n",
    "\n",
    "    head['link_time_min_sum'] = head.index.map(gl['link_time_min'].sum())\n",
    "    head['link_time_min_avg'] = head.index.map(gl['link_time_min'].mean())\n",
    "    head['link_time_min_std'] = head.index.map(gl['link_time_min'].std())\n",
    "    head['link_time_min_min'] = head.index.map(gl['link_time_min'].min())\n",
    "    head['link_time_min_max'] = head.index.map(gl['link_time_min'].max())\n",
    "\n",
    "    head['link_time_count_sum'] = head.index.map(gl['link_time_count'].sum())\n",
    "    head['link_time_count_avg'] = head.index.map(gl['link_time_count'].mean())\n",
    "    head['link_time_count_std'] = head.index.map(gl['link_time_count'].std())\n",
    "    head['link_time_count_min'] = head.index.map(gl['link_time_count'].min())\n",
    "    head['link_time_count_max'] = head.index.map(gl['link_time_count'].max())\n",
    "# -----------------------------------------------------------------------------\n",
    "    \n",
    "    head['time_delay_max'] = head.index.map(gl['cum_time'].max())\n",
    "    head['time_delay_avg'] = head.index.map(gl['cum_time'].mean())\n",
    "    head['time_delay_std'] = head.index.map(gl['cum_time'].std())\n",
    "    \n",
    "    gl = link.groupby(['order_id','link_current_status'])['link_time']\n",
    "    head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('current_no_'))\n",
    "    head = head.join(gl.sum().unstack(level=-1,fill_value=0).add_prefix('current_time_sum_'))\n",
    "    head = head.join(gl.mean().unstack(level=-1,fill_value=0).add_prefix('current_time_avg_'))\n",
    "    head = head.join(gl.std().unstack(level=-1,fill_value=0).add_prefix('current_time_std_'))\n",
    "    head = head.join(gl.max().unstack(level=-1,fill_value=0).add_prefix('current_time_max_'))\n",
    "    head = head.join(gl.min().unstack(level=-1,fill_value=0).add_prefix('current_time_min_'))\n",
    "\n",
    "    gl = link.groupby(['order_id','next_link'])['link_time']\n",
    "    head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('next_no_'))\n",
    "    head = head.join(gl.sum().unstack(level=-1,fill_value=0).add_prefix('next_time_sum_'))\n",
    "    head = head.join(gl.mean().unstack(level=-1,fill_value=0).add_prefix('next_time_avg_'))\n",
    "    head = head.join(gl.std().unstack(level=-1,fill_value=0).add_prefix('next_time_std_'))\n",
    "    head = head.join(gl.max().unstack(level=-1,fill_value=0).add_prefix('next_time_max_'))\n",
    "    head = head.join(gl.min().unstack(level=-1,fill_value=0).add_prefix('next_time_min_'))\n",
    "\n",
    "    gl = link.groupby(['order_id','link_type'])['link_time']\n",
    "    head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('type_no_'))\n",
    "    head = head.join(gl.sum().unstack(level=-1,fill_value=0).add_prefix('type_time_sum_'))\n",
    "    head = head.join(gl.mean().unstack(level=-1,fill_value=0).add_prefix('type_time_avg_'))\n",
    "    head = head.join(gl.std().unstack(level=-1,fill_value=0).add_prefix('type_time_std_'))\n",
    "    head = head.join(gl.max().unstack(level=-1,fill_value=0).add_prefix('type_time_max_'))\n",
    "    head = head.join(gl.min().unstack(level=-1,fill_value=0).add_prefix('type_time_min_'))\n",
    "    \n",
    "#     gl = link.groupby(['order_id','link_arrival_status'])['link_time']\n",
    "#     head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('arrival_no_'))\n",
    "#     head = head.join(gl.sum().unstack(level=-1,fill_value=0).add_prefix('arrival_time_sum_'))\n",
    "#     head = head.join(gl.mean().unstack(level=-1,fill_value=0).add_prefix('arrival_time_avg_'))\n",
    "#     head = head.join(gl.std().unstack(level=-1,fill_value=0).add_prefix('arrival_time_std_'))\n",
    "#     head = head.join(gl.max().unstack(level=-1,fill_value=0).add_prefix('arrival_time_max_'))\n",
    "#     head = head.join(gl.min().unstack(level=-1,fill_value=0).add_prefix('arrival_time_min_'))\n",
    "\n",
    "########################\n",
    "###cross features\n",
    "########################\n",
    "    gc = cross.groupby('order_id') \n",
    "    head['cross_no'] = head.index.map(gc.size())\n",
    "    head['cross_sum'] = head.index.map(gc['link_time'].sum())\n",
    "    head['cross_avg'] = head.index.map(gc['link_time'].mean())\n",
    "    head['cross_std'] = head.index.map(gc['link_time'].std())\n",
    "    head['cross_max'] = head.index.map(gc['link_time'].max())\n",
    "    head['cross_min'] = head.index.map(gc['link_time'].min())\n",
    "#     head['cross_ratio'] = head['cross_sum']/head['simple_eta']\n",
    "\n",
    "    head = head.fillna(0).reset_index().set_index('slice_id')\n",
    "    gl = link.groupby(['slice_id','link_current_status'])\n",
    "    head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('link_status_no_'))\n",
    "    head = head.fillna(0).reset_index()\n",
    "\n",
    "    if(task=='train'):\n",
    "        head['date'] = f[:8]\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = pd.read_table(link_dir+'nextlinks.txt',sep=' ',names=['link_id','next_link'])\n",
    "network['next_link_no'] = network['next_link'].apply(lambda z: 4 if len(list(z.split(',')))>4 else len(list(z.split(','))))\n",
    "nl = network.set_index('link_id')['next_link_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 std       avg     min      max   count\n",
       "Unnamed: 0                                             \n",
       "157298      3.142503  5.023350  0.3210  14.4000   137.0\n",
       "511622      1.827190  3.757110  0.0000  16.5600   144.0\n",
       "524542      1.259789  2.886754  0.0000  12.9600   155.0\n",
       "97293       1.929949  4.649475  2.0020  20.8800   161.0\n",
       "273845      1.726321  4.192694  2.5297  18.7200   160.0\n",
       "556273      2.016241  4.887015  2.6341  21.6000   162.0\n",
       "73842       1.432121  3.350948  0.2437  14.4000   165.0\n",
       "101675      1.427683  3.406463  2.0000  14.4000   166.0\n",
       "325444      0.093270  0.699338  0.0000   1.7053  5337.0\n",
       "134737      0.389939  4.034132  0.3265   8.8364  5338.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>std</th>\n      <th>avg</th>\n      <th>min</th>\n      <th>max</th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>Unnamed: 0</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>157298</th>\n      <td>3.142503</td>\n      <td>5.023350</td>\n      <td>0.3210</td>\n      <td>14.4000</td>\n      <td>137.0</td>\n    </tr>\n    <tr>\n      <th>511622</th>\n      <td>1.827190</td>\n      <td>3.757110</td>\n      <td>0.0000</td>\n      <td>16.5600</td>\n      <td>144.0</td>\n    </tr>\n    <tr>\n      <th>524542</th>\n      <td>1.259789</td>\n      <td>2.886754</td>\n      <td>0.0000</td>\n      <td>12.9600</td>\n      <td>155.0</td>\n    </tr>\n    <tr>\n      <th>97293</th>\n      <td>1.929949</td>\n      <td>4.649475</td>\n      <td>2.0020</td>\n      <td>20.8800</td>\n      <td>161.0</td>\n    </tr>\n    <tr>\n      <th>273845</th>\n      <td>1.726321</td>\n      <td>4.192694</td>\n      <td>2.5297</td>\n      <td>18.7200</td>\n      <td>160.0</td>\n    </tr>\n    <tr>\n      <th>556273</th>\n      <td>2.016241</td>\n      <td>4.887015</td>\n      <td>2.6341</td>\n      <td>21.6000</td>\n      <td>162.0</td>\n    </tr>\n    <tr>\n      <th>73842</th>\n      <td>1.432121</td>\n      <td>3.350948</td>\n      <td>0.2437</td>\n      <td>14.4000</td>\n      <td>165.0</td>\n    </tr>\n    <tr>\n      <th>101675</th>\n      <td>1.427683</td>\n      <td>3.406463</td>\n      <td>2.0000</td>\n      <td>14.4000</td>\n      <td>166.0</td>\n    </tr>\n    <tr>\n      <th>325444</th>\n      <td>0.093270</td>\n      <td>0.699338</td>\n      <td>0.0000</td>\n      <td>1.7053</td>\n      <td>5337.0</td>\n    </tr>\n    <tr>\n      <th>134737</th>\n      <td>0.389939</td>\n      <td>4.034132</td>\n      <td>0.3265</td>\n      <td>8.8364</td>\n      <td>5338.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "link_attr = pd.read_csv(link_dir+'link_attr.csv')\n",
    "link_attr.set_index('Unnamed: 0',inplace=True)\n",
    "\n",
    "link_time_attr = pd.read_csv(link_dir+'link_time_attr.csv')\n",
    "link_time_attr.set_index('Unnamed: 0',inplace=True)\n",
    "\n",
    "driver_attr = pd.read_csv(link_dir+'driver_attr.csv')\n",
    "driver_attr.set_index('Unnamed: 0',inplace=True)\n",
    "\n",
    "link_time_attr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "31it [30:01, 58.12s/it]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FILES = ['202008'+str(i).zfill(2)+'.csv' for i in range(1,32)]\n",
    "\n",
    "for i,f in tqdm(enumerate(TRAIN_FILES)):    \n",
    "    head = pd.read_csv(train_dir+'train_head/'+f)\n",
    "    path = pd.read_csv(train_dir+'train_path/'+f)\n",
    "\n",
    "    if(head.shape[0]==0):\n",
    "        continue\n",
    "    if i==0:\n",
    "        train = gen_feats('train', head, path, nl, link_attr, link_time_attr, driver_attr)\n",
    "    else:\n",
    "        train = train.append(gen_feats('train', head, path, nl, link_attr, link_time_attr, driver_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = pd.read_csv(test_dir+'test_head.csv')\n",
    "path = pd.read_csv(test_dir+'test_path.csv')\n",
    "\n",
    "test = gen_feats('test', head, path, nl, link_attr, link_time_attr, driver_attr)\n",
    "test['date'] = '20200901'"
   ]
  },
  {
   "source": [
    "生成并保存 train set 和 test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train['date'] = pd.to_datetime(train['date'].astype('str'))\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['day_bias'] = (df['date']-pd.Timestamp(2020,8,1)).dt.days\n",
    "    df['weekday'] = df['date'].dt.weekday<5\n",
    "    df['cross_no'] = df['cross_no']*(df['cross_sum']>0)\n",
    "    df['link_length'] = df['distance']/df['link_no']\n",
    "    df['log_distance'] = np.log(df['distance'])\n",
    "    df['log_simple_eta'] = np.log(df['simple_eta'])\n",
    "    \n",
    "    for i in range(5):\n",
    "        df['current_no_ratio_'+str(i)] = df['current_no_'+str(i)]/df['link_no']\n",
    "        df['current_time_ratio_'+str(i)] = df['current_time_sum_'+str(i)]/(df['simple_eta']-df['cross_sum'])\n",
    "        df['next_no_ratio_'+str(i)] = df['next_no_'+str(i)]/df['link_no']\n",
    "        df['next_time_ratio_'+str(i)] = df['next_time_sum_'+str(i)]/(df['simple_eta']-df['cross_sum'])\n",
    "    for i in range(4):\n",
    "        df['type_no_ratio_'+str(i)] = df['type_no_'+str(i)]/df['link_no']\n",
    "        df['type_time_ratio_'+str(i)] = df['type_time_sum_'+str(i)]/(df['simple_eta']-df['cross_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "    \n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "print(train.info())\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(train_dir+'train.pkl')\n",
    "test.to_pickle(test_dir+'test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dz",
   "language": "python",
   "name": "dz"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}