{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os, random, csv, datetime, json\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import argparse\r\n",
    "import time\r\n",
    "import pickle\r\n",
    "from tqdm import tqdm\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "train_dir=\"../data/train/\"\r\n",
    "test_dir=\"../data/test/\"\r\n",
    "\r\n",
    "link_dir=\"../data/\"\r\n",
    "\r\n",
    "%pwd"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/cseadmin/undergrad1/dz/GISCUP2021/model'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def gen_feats(task, head, path, nl, link_attr, link_time_attr, driver_attr):\r\n",
    "########################\r\n",
    "###head features\r\n",
    "########################\r\n",
    "    head['speed'] = head['distance']/head['simple_eta']\r\n",
    "    head['speed_avg'] = head['slice_id'].map(head.groupby('slice_id')['speed'].mean())\r\n",
    "    head['speed_std'] = head['slice_id'].map(head.groupby('slice_id')['speed'].std())\r\n",
    "    head['speed_min'] = head['slice_id'].map(head.groupby('slice_id')['speed'].min())\r\n",
    "    head['speed_max'] = head['slice_id'].map(head.groupby('slice_id')['speed'].max())\r\n",
    "    head['volume'] = head['slice_id'].map(head.groupby('slice_id').size())\r\n",
    "    head['old_driver'] = head['driver_id'].map(head.groupby('driver_id').size())\r\n",
    "\r\n",
    "    head['driver_avg'] = head['driver_id'].map(driver_attr['avg'])\r\n",
    "    head['driver_std'] = head['driver_id'].map(driver_attr['std'])\r\n",
    "    head['driver_min'] = head['driver_id'].map(driver_attr['min'])\r\n",
    "    head['driver_max'] = head['driver_id'].map(driver_attr['max'])\r\n",
    "    head['driver_cnt'] = head['driver_id'].map(driver_attr['count'])\r\n",
    "    \r\n",
    "    head.set_index('order_id', inplace=True)\r\n",
    "    \r\n",
    "    path['link_eta'] = path['link_time']*path['link_ratio']\r\n",
    "    path['cum_time'] = (path.groupby('order_id')['link_eta'].cumsum()/300).astype('int')\r\n",
    "    link = path[path['cross_flag']==0]\r\n",
    "    cross = path[path['cross_flag']==1]\r\n",
    "########################\r\n",
    "###link features\r\n",
    "########################\r\n",
    "    link = link[link['link_time']>0]\r\n",
    "    link['link_id'] = link['link_id'].astype('int')\r\n",
    "    link['link_type'] = link['link_id'].map(link_attr['link_label']).fillna(0).astype('int')\r\n",
    "    link['next_link'] = link['link_id'].map(nl).fillna(0).astype('int')\r\n",
    "    link['slice_id'] = link['order_id'].map(head['slice_id'])\r\n",
    "\r\n",
    "    link[\"link_time_std\"] = link[\"link_id\"].map(link_time_attr[\"std\"])\r\n",
    "    link[\"link_time_avg\"] = link[\"link_id\"].map(link_time_attr[\"avg\"])\r\n",
    "    link[\"link_time_min\"] = link[\"link_id\"].map(link_time_attr[\"min\"])\r\n",
    "    link[\"link_time_max\"] = link[\"link_id\"].map(link_time_attr[\"max\"])\r\n",
    "    link[\"link_time_count\"] = link[\"link_id\"].map(link_time_attr[\"count\"])\r\n",
    "    link['link_time_type'] = link['link_id'].map(link_time_attr['label']).fillna(0).astype('int')\r\n",
    "    \r\n",
    "    gl = link.groupby('order_id')\r\n",
    "    head['link_no'] = head.index.map(gl.size())\r\n",
    "    head['link_time_sum'] = head.index.map(gl['link_eta'].sum())\r\n",
    "    head['link_time_avg'] = head.index.map(gl['link_time'].mean())\r\n",
    "    head['link_time_std'] = head.index.map(gl['link_time'].std())\r\n",
    "    head['link_time_max'] = head.index.map(gl['link_time'].max())\r\n",
    "    head['link_time_min'] = head.index.map(gl['link_time'].min())\r\n",
    "\r\n",
    "# -----------------------------------------------------------------------------\r\n",
    "    head['link_time_std_sum'] = head.index.map(gl['link_time_std'].sum())\r\n",
    "    head['link_time_std_avg'] = head.index.map(gl['link_time_std'].mean())\r\n",
    "    head['link_time_std_std'] = head.index.map(gl['link_time_std'].std())\r\n",
    "    head['link_time_std_min'] = head.index.map(gl['link_time_std'].min())\r\n",
    "    head['link_time_std_max'] = head.index.map(gl['link_time_std'].max())\r\n",
    "\r\n",
    "    head['link_time_avg_sum'] = head.index.map(gl['link_time_avg'].sum())\r\n",
    "    head['link_time_avg_avg'] = head.index.map(gl['link_time_avg'].mean())\r\n",
    "    head['link_time_avg_std'] = head.index.map(gl['link_time_avg'].std())\r\n",
    "    head['link_time_avg_min'] = head.index.map(gl['link_time_avg'].min())\r\n",
    "    head['link_time_avg_max'] = head.index.map(gl['link_time_avg'].max())\r\n",
    "\r\n",
    "    head['link_time_max_sum'] = head.index.map(gl['link_time_max'].sum())\r\n",
    "    head['link_time_max_avg'] = head.index.map(gl['link_time_max'].mean())\r\n",
    "    head['link_time_max_std'] = head.index.map(gl['link_time_max'].std())\r\n",
    "    head['link_time_max_min'] = head.index.map(gl['link_time_max'].min())\r\n",
    "    head['link_time_max_max'] = head.index.map(gl['link_time_max'].max())\r\n",
    "\r\n",
    "    head['link_time_min_sum'] = head.index.map(gl['link_time_min'].sum())\r\n",
    "    head['link_time_min_avg'] = head.index.map(gl['link_time_min'].mean())\r\n",
    "    head['link_time_min_std'] = head.index.map(gl['link_time_min'].std())\r\n",
    "    head['link_time_min_min'] = head.index.map(gl['link_time_min'].min())\r\n",
    "    head['link_time_min_max'] = head.index.map(gl['link_time_min'].max())\r\n",
    "\r\n",
    "    head['link_time_count_sum'] = head.index.map(gl['link_time_count'].sum())\r\n",
    "    head['link_time_count_avg'] = head.index.map(gl['link_time_count'].mean())\r\n",
    "    head['link_time_count_std'] = head.index.map(gl['link_time_count'].std())\r\n",
    "    head['link_time_count_min'] = head.index.map(gl['link_time_count'].min())\r\n",
    "    head['link_time_count_max'] = head.index.map(gl['link_time_count'].max())\r\n",
    "# -----------------------------------------------------------------------------\r\n",
    "    \r\n",
    "    head['time_delay_max'] = head.index.map(gl['cum_time'].max())\r\n",
    "    head['time_delay_avg'] = head.index.map(gl['cum_time'].mean())\r\n",
    "    head['time_delay_std'] = head.index.map(gl['cum_time'].std())\r\n",
    "    \r\n",
    "    gl = link.groupby(['order_id','link_current_status'])['link_time']\r\n",
    "    head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('current_no_'))\r\n",
    "    head = head.join(gl.sum().unstack(level=-1,fill_value=0).add_prefix('current_time_sum_'))\r\n",
    "    head = head.join(gl.mean().unstack(level=-1,fill_value=0).add_prefix('current_time_avg_'))\r\n",
    "    head = head.join(gl.std().unstack(level=-1,fill_value=0).add_prefix('current_time_std_'))\r\n",
    "    head = head.join(gl.max().unstack(level=-1,fill_value=0).add_prefix('current_time_max_'))\r\n",
    "    head = head.join(gl.min().unstack(level=-1,fill_value=0).add_prefix('current_time_min_'))\r\n",
    "\r\n",
    "    gl = link.groupby(['order_id','next_link'])['link_time']\r\n",
    "    head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('next_no_'))\r\n",
    "    head = head.join(gl.sum().unstack(level=-1,fill_value=0).add_prefix('next_time_sum_'))\r\n",
    "    head = head.join(gl.mean().unstack(level=-1,fill_value=0).add_prefix('next_time_avg_'))\r\n",
    "    head = head.join(gl.std().unstack(level=-1,fill_value=0).add_prefix('next_time_std_'))\r\n",
    "    head = head.join(gl.max().unstack(level=-1,fill_value=0).add_prefix('next_time_max_'))\r\n",
    "    head = head.join(gl.min().unstack(level=-1,fill_value=0).add_prefix('next_time_min_'))\r\n",
    "\r\n",
    "    gl = link.groupby(['order_id','link_type'])['link_time']\r\n",
    "    head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('type_no_'))\r\n",
    "    head = head.join(gl.sum().unstack(level=-1,fill_value=0).add_prefix('type_time_sum_'))\r\n",
    "    head = head.join(gl.mean().unstack(level=-1,fill_value=0).add_prefix('type_time_avg_'))\r\n",
    "    head = head.join(gl.std().unstack(level=-1,fill_value=0).add_prefix('type_time_std_'))\r\n",
    "    head = head.join(gl.max().unstack(level=-1,fill_value=0).add_prefix('type_time_max_'))\r\n",
    "    head = head.join(gl.min().unstack(level=-1,fill_value=0).add_prefix('type_time_min_'))\r\n",
    "\r\n",
    "    gl = link.groupby(['order_id','link_time_type'])['link_time']\r\n",
    "    head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('time_type_no_'))\r\n",
    "    head = head.join(gl.sum().unstack(level=-1,fill_value=0).add_prefix('time_type_time_sum_'))\r\n",
    "    head = head.join(gl.mean().unstack(level=-1,fill_value=0).add_prefix('time_type_time_avg_'))\r\n",
    "    head = head.join(gl.std().unstack(level=-1,fill_value=0).add_prefix('time_type_time_std_'))\r\n",
    "    head = head.join(gl.max().unstack(level=-1,fill_value=0).add_prefix('time_type_time_max_'))\r\n",
    "    head = head.join(gl.min().unstack(level=-1,fill_value=0).add_prefix('time_type_time_min_'))\r\n",
    "    \r\n",
    "#     gl = link.groupby(['order_id','link_arrival_status'])['link_time']\r\n",
    "#     head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('arrival_no_'))\r\n",
    "#     head = head.join(gl.sum().unstack(level=-1,fill_value=0).add_prefix('arrival_time_sum_'))\r\n",
    "#     head = head.join(gl.mean().unstack(level=-1,fill_value=0).add_prefix('arrival_time_avg_'))\r\n",
    "#     head = head.join(gl.std().unstack(level=-1,fill_value=0).add_prefix('arrival_time_std_'))\r\n",
    "#     head = head.join(gl.max().unstack(level=-1,fill_value=0).add_prefix('arrival_time_max_'))\r\n",
    "#     head = head.join(gl.min().unstack(level=-1,fill_value=0).add_prefix('arrival_time_min_'))\r\n",
    "\r\n",
    "########################\r\n",
    "###cross features\r\n",
    "########################\r\n",
    "    gc = cross.groupby('order_id') \r\n",
    "    head['cross_no'] = head.index.map(gc.size())\r\n",
    "    head['cross_sum'] = head.index.map(gc['link_time'].sum())\r\n",
    "    head['cross_avg'] = head.index.map(gc['link_time'].mean())\r\n",
    "    head['cross_std'] = head.index.map(gc['link_time'].std())\r\n",
    "    head['cross_max'] = head.index.map(gc['link_time'].max())\r\n",
    "    head['cross_min'] = head.index.map(gc['link_time'].min())\r\n",
    "#     head['cross_ratio'] = head['cross_sum']/head['simple_eta']\r\n",
    "\r\n",
    "    head = head.fillna(0).reset_index().set_index('slice_id')\r\n",
    "    gl = link.groupby(['slice_id','link_current_status'])\r\n",
    "    head = head.join(gl.size().unstack(level=-1,fill_value=0).add_prefix('link_status_no_'))\r\n",
    "    head = head.fillna(0).reset_index()\r\n",
    "\r\n",
    "    if(task=='train'):\r\n",
    "        head['date'] = f[:8]\r\n",
    "    return head"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "network = pd.read_table(link_dir+'nextlinks.txt',sep=' ',names=['link_id','next_link'])\r\n",
    "network['next_link_no'] = network['next_link'].apply(lambda z: 4 if len(list(z.split(',')))>4 else len(list(z.split(','))))\r\n",
    "nl = network.set_index('link_id')['next_link_no']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "link_attr = pd.read_csv(link_dir+'link_attr.csv')\r\n",
    "link_attr.set_index('Unnamed: 0',inplace=True)\r\n",
    "\r\n",
    "link_time_attr = pd.read_csv(link_dir+'link_time_attr.csv')\r\n",
    "link_time_attr.set_index('Unnamed: 0',inplace=True)\r\n",
    "\r\n",
    "driver_attr = pd.read_csv(link_dir+'driver_attr.csv')\r\n",
    "driver_attr.set_index('Unnamed: 0',inplace=True)\r\n",
    "\r\n",
    "link_time_attr.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 std       avg     min      max   count  label\n",
       "Unnamed: 0                                                    \n",
       "157298      3.142503  5.023350  0.3210  14.4000   137.0      1\n",
       "511622      1.761514  3.837049  0.2957  16.5600   141.0      1\n",
       "524542      1.242140  2.905499  0.4696  12.9600   154.0      1\n",
       "97293       1.929949  4.649475  2.0020  20.8800   161.0      1\n",
       "273845      1.726321  4.192694  2.5297  18.7200   160.0      4\n",
       "556273      2.016241  4.887015  2.6341  21.6000   162.0      4\n",
       "73842       1.432121  3.350948  0.2437  14.4000   165.0      1\n",
       "101675      1.427683  3.406463  2.0000  14.4000   166.0      1\n",
       "325444      0.092299  0.699600  0.0711   1.7053  5335.0      1\n",
       "134737      0.389939  4.034132  0.3265   8.8364  5338.0      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>std</th>\n",
       "      <th>avg</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157298</th>\n",
       "      <td>3.142503</td>\n",
       "      <td>5.023350</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>14.4000</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511622</th>\n",
       "      <td>1.761514</td>\n",
       "      <td>3.837049</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>16.5600</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524542</th>\n",
       "      <td>1.242140</td>\n",
       "      <td>2.905499</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>12.9600</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97293</th>\n",
       "      <td>1.929949</td>\n",
       "      <td>4.649475</td>\n",
       "      <td>2.0020</td>\n",
       "      <td>20.8800</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273845</th>\n",
       "      <td>1.726321</td>\n",
       "      <td>4.192694</td>\n",
       "      <td>2.5297</td>\n",
       "      <td>18.7200</td>\n",
       "      <td>160.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556273</th>\n",
       "      <td>2.016241</td>\n",
       "      <td>4.887015</td>\n",
       "      <td>2.6341</td>\n",
       "      <td>21.6000</td>\n",
       "      <td>162.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73842</th>\n",
       "      <td>1.432121</td>\n",
       "      <td>3.350948</td>\n",
       "      <td>0.2437</td>\n",
       "      <td>14.4000</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101675</th>\n",
       "      <td>1.427683</td>\n",
       "      <td>3.406463</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>14.4000</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325444</th>\n",
       "      <td>0.092299</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>1.7053</td>\n",
       "      <td>5335.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134737</th>\n",
       "      <td>0.389939</td>\n",
       "      <td>4.034132</td>\n",
       "      <td>0.3265</td>\n",
       "      <td>8.8364</td>\n",
       "      <td>5338.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "TRAIN_FILES = ['202008'+str(i).zfill(2)+'.csv' for i in range(1,32)]\r\n",
    "\r\n",
    "for i,f in tqdm(enumerate(TRAIN_FILES)):    \r\n",
    "    head = pd.read_csv(train_dir+'train_head/'+f)\r\n",
    "    path = pd.read_csv(train_dir+'train_path/'+f)\r\n",
    "\r\n",
    "    if(head.shape[0]==0):\r\n",
    "        continue\r\n",
    "    if i==0:\r\n",
    "        train = gen_feats('train', head, path, nl, link_attr, link_time_attr, driver_attr)\r\n",
    "    else:\r\n",
    "        train = train.append(gen_feats('train', head, path, nl, link_attr, link_time_attr, driver_attr))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "head = pd.read_csv(test_dir+'test_head.csv')\r\n",
    "path = pd.read_csv(test_dir+'test_path.csv')\r\n",
    "\r\n",
    "test = gen_feats('test', head, path, nl, link_attr, link_time_attr, driver_attr)\r\n",
    "test['date'] = '20200901'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "生成并保存 train set 和 test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "\r\n",
    "train['date'] = pd.to_datetime(train['date'].astype('str'))\r\n",
    "test['date'] = pd.to_datetime(test['date'])\r\n",
    "\r\n",
    "for df in [train, test]:\r\n",
    "    df['day_bias'] = (df['date']-pd.Timestamp(2020,8,1)).dt.days\r\n",
    "    df['weekday'] = df['date'].dt.weekday<5\r\n",
    "    df['cross_no'] = df['cross_no']*(df['cross_sum']>0)\r\n",
    "    df['link_length'] = df['distance']/df['link_no']\r\n",
    "    df['log_distance'] = np.log(df['distance'])\r\n",
    "    df['log_simple_eta'] = np.log(df['simple_eta'])\r\n",
    "    \r\n",
    "    for i in range(5):\r\n",
    "        df['current_no_ratio_'+str(i)] = df['current_no_'+str(i)]/df['link_no']\r\n",
    "        df['current_time_ratio_'+str(i)] = df['current_time_sum_'+str(i)]/(df['simple_eta']-df['cross_sum'])\r\n",
    "        df['next_no_ratio_'+str(i)] = df['next_no_'+str(i)]/df['link_no']\r\n",
    "        df['next_time_ratio_'+str(i)] = df['next_time_sum_'+str(i)]/(df['simple_eta']-df['cross_sum'])\r\n",
    "    for i in range(4):\r\n",
    "        df['type_no_ratio_'+str(i)] = df['type_no_'+str(i)]/df['link_no']\r\n",
    "        df['type_time_ratio_'+str(i)] = df['type_time_sum_'+str(i)]/(df['simple_eta']-df['cross_sum'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.34 s, sys: 788 ms, total: 4.12 s\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "\r\n",
    "def reduce_mem_usage(df, verbose=True):\r\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\r\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\r\n",
    "    for col in df.columns:\r\n",
    "        col_type = df[col].dtypes\r\n",
    "        if col_type in numerics:\r\n",
    "            c_min = df[col].min()\r\n",
    "            c_max = df[col].max()\r\n",
    "            if str(col_type)[:3] == 'int':\r\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\r\n",
    "                    df[col] = df[col].astype(np.int8)\r\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\r\n",
    "                    df[col] = df[col].astype(np.int16)\r\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\r\n",
    "                    df[col] = df[col].astype(np.int32)\r\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\r\n",
    "                    df[col] = df[col].astype(np.int64)\r\n",
    "            else:\r\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\r\n",
    "                    df[col] = df[col].astype(np.float16)\r\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\r\n",
    "                    df[col] = df[col].astype(np.float32)\r\n",
    "                else:\r\n",
    "                    df[col] = df[col].astype(np.float64)\r\n",
    "\r\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\r\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\r\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\r\n",
    "\r\n",
    "    return df\r\n",
    "    \r\n",
    "train = reduce_mem_usage(train)\r\n",
    "test = reduce_mem_usage(test)\r\n",
    "print(train.info())\r\n",
    "print(test.info())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory usage after optimization is: 3192.84 MB\n",
      "Decreased by 73.3%\n",
      "Memory usage after optimization is: 101.65 MB\n",
      "Decreased by 74.3%\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8651005 entries, 0 to 291609\n",
      "Columns: 181 entries, slice_id to type_time_ratio_3\n",
      "dtypes: bool(1), datetime64[ns](1), float16(142), float32(5), int16(22), int32(4), int8(6)\n",
      "memory usage: 3.1 GB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 288076 entries, 0 to 288075\n",
      "Columns: 181 entries, slice_id to type_time_ratio_3\n",
      "dtypes: bool(1), datetime64[ns](1), float16(147), float32(3), int16(17), int32(3), int8(9)\n",
      "memory usage: 101.7 MB\n",
      "None\n",
      "CPU times: user 38.9 s, sys: 13.8 s, total: 52.6 s\n",
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train.to_pickle(train_dir+'train.pkl')\r\n",
    "test.to_pickle(test_dir+'test.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dz",
   "language": "python",
   "name": "dz"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}